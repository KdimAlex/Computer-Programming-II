{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KdimAlex/Computer-Programming-II/blob/master/Academic_AI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYpmmfQ-ZbIh",
        "outputId": "03197ca1-a215-4d98-be21-0d03b47b84cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.9/41.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade langchain openai  -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5D5W_BeBZf5X"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XbL5zdf9aG8a",
        "outputId": "217dd844-09df-476d-a010-62e2876db919"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m358.9/358.9 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.1/275.1 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.5/115.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m78.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m237.4/237.4 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m91.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.8/101.8 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m106.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m80.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m96.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.2/112.2 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.5/47.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.5/112.5 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m59.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for python-pptx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for ebooklib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for olefile (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.0/131.0 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for detectron2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install unstructured -q\n",
        "!pip install unstructured[local-inference] -q\n",
        "!pip install detectron2@git+https://github.com/facebookresearch/detectron2.git@v0.6#egg=detectron2 -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwalYTVZoRlH",
        "outputId": "4b6f14bc-1286-4e31-bb07-a1744304d83d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 18 not upgraded.\n",
            "Need to get 186 kB of archives.\n",
            "After this operation, 696 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.2 [186 kB]\n",
            "Fetched 186 kB in 1s (313 kB/s)\n",
            "Selecting previously unselected package poppler-utils.\n",
            "(Reading database ... 120874 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.2_amd64.deb ...\n",
            "Unpacking poppler-utils (22.02.0-2ubuntu0.2) ...\n",
            "Setting up poppler-utils (22.02.0-2ubuntu0.2) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ],
      "source": [
        "!apt-get install poppler-utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fulYnj9nZr3n",
        "outputId": "1ee9ae17-849c-4547-b727-e16c9c7d647c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "from langchain.document_loaders import DirectoryLoader\n",
        "\n",
        "directory = '/content/data'\n",
        "\n",
        "def load_docs(directory):\n",
        "  loader = DirectoryLoader(directory)\n",
        "  documents = loader.load()\n",
        "  return documents\n",
        "\n",
        "documents = load_docs(directory)\n",
        "len(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5lF8jA6xZ0Hm",
        "outputId": "c30ce03e-0b86-4da9-b80e-d83d3123ba93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "def split_docs(documents,chunk_size=1000,chunk_overlap=20):\n",
        "  text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "  docs = text_splitter.split_documents(documents)\n",
        "  return docs\n",
        "\n",
        "docs = split_docs(documents)\n",
        "print(len(docs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "tVcFjgHJZ8S9",
        "outputId": "a534d399-6d3b-45de-a94e-8123b6be93c2"
      },
      "outputs": [
        {
          "ename": "IndexError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-2e9f92d2ae21>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#TEST TO SEE DOCUMENTS READ IN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ],
      "source": [
        "#TEST TO SEE DOCUMENTS READ IN\n",
        "print(docs[0].page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGXlt51N2xyJ",
        "outputId": "43bb9fb1-19e3-4fbb-c7b8-e97bed3059ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(5) “Falling action” or the “Denouement”: you have to bring the audience down\n",
            "\n",
            "(6) We now know feeling: “the great ah”\n",
            "\n",
            "d) US vs Europe\n",
            "\n",
            "(1) Europe focuses on character\n",
            "\n",
            "(2) US focuses on action\n",
            "\n",
            "(a) Likely successful because you don’t have to understand the language, as\n",
            "\n",
            "the US was historically colonized and settled as a melting pot\n",
            "\n",
            "II. George Melies:\n",
            "\n",
            "A. Good at making sets and special eﬀects using models and costumes shown in HUGO\n",
            "\n",
            "B. Made ﬁlms like theater\n",
            "\n",
            "C. Prop usage\n",
            "\n",
            "D. Static camera\n",
            "\n",
            "1. Pushed props to ward the camera—like theater—camera stars across orchestra pit\n",
            "\n",
            "E. “A trip to the Moon”\n",
            "\n",
            "1. First ﬁlm about going to the moon\n",
            "\n",
            "III. Edison steals a copy of “A trip to the moon”\n",
            "\n",
            "A. Discovers how to make copies of original movies\n",
            "\n",
            "B. Never pays Melies\n",
            "\n",
            "IV. Nickelodeons/ ﬂickers in US\n",
            "\n",
            "V. Bicycle prints: only one copy get shuttled around theaters on bicycles.\n",
            "\n",
            "VI. Edison’s best ﬁlm: “A train robbery”\n"
          ]
        }
      ],
      "source": [
        "#TEST TO SEE DOCUMENTS READ IN\n",
        "print(docs[5].page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PiPwt-FaYwl",
        "outputId": "7c7e5a61-e558-4b84-e43c-4a24824545ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install tiktoken -q\n",
        "#required for open ai embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5GY9voPa0av",
        "outputId": "5045529b-97b6-4654-d00b-4f18f874cadd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain/embeddings/openai.py:217: UserWarning: WARNING! model_name is not default parameter.\n",
            "                    model_name was transferred to model_kwargs.\n",
            "                    Please confirm that model_name is what you intended.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1536"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "import openai\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model_name=\"ada\")\n",
        "\n",
        "query_result = embeddings.embed_query(\"Hello world\")\n",
        "len(query_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXhIY5SrrRec",
        "outputId": "85d4d2ac-b6a9-4495-8971-fffe99d3066d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.4/179.4 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.4/300.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install pinecone-client -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfIpYLV-acks",
        "outputId": "ebb3090b-f721-4dca-bd40-f08d15dda4dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pinecone/index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm\n"
          ]
        }
      ],
      "source": [
        "import pinecone\n",
        "from langchain.vectorstores import Pinecone\n",
        "# initialize pinecone\n",
        "pinecone.init(\n",
        "    api_key=\"d9922e4d-16c0-4288-936f-b73456337e51\",  # find at app.pinecone.io\n",
        "    environment=\"us-west1-gcp-free\"  # next to api key in console\n",
        ")\n",
        "\n",
        "index_name = \"personal-academic-chatbot\"\n",
        "\n",
        "index = Pinecone.from_documents(docs, embeddings, index_name=index_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "o5r7YLpbchAD",
        "outputId": "e1734a41-c365-4edd-a52a-18173b4dc2c3"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-057398d561c9>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msimilar_docs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"What would you like to ask? \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0msimilar_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_similiar_docs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0msimilar_docs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "#TEST RUN THAT SIMILAR DOCUMENTS GET RETURNED FROM PINECONE\n",
        "def get_similiar_docs(query,k=3,score=False):\n",
        "  if score:\n",
        "    similar_docs = index.similarity_search_with_score(query,k=k)\n",
        "  else:\n",
        "    similar_docs = index.similarity_search(query,k=k)\n",
        "  return similar_docs\n",
        "\n",
        "query = input(\"What would you like to ask? \")\n",
        "similar_docs = get_similiar_docs(query)\n",
        "similar_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DuevPx4dbI4W",
        "outputId": "fdbb9f06-ba53-4c6c-8233-f15dea00db4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain/llms/openai.py:216: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain/llms/openai.py:811: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from langchain.llms import OpenAI\n",
        "\n",
        "# model_name = \"text-davinci-003\"\n",
        "# model_name = \"gpt-3.5-turbo\"\n",
        "model_name = \"gpt-3.5-turbo\"\n",
        "llm = OpenAI(model_name=model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8jDQXra2_X8E",
        "outputId": "4ada8444-6507-4040-c33a-59927d7920e8"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Documents Information Being Sent to LLM: \n"
          ]
        },
        {
          "data": {
            "text/markdown": "### Document 1:\n**Type of doc:** <class 'langchain.schema.document.Document'>\n**File Path:** /content/data/pdfcoffee.com-fpmkathy6th-edpdf.pdf\n**Content:**\n\n3.\n\n4.\n",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "### Document 2:\n**Type of doc:** <class 'langchain.schema.document.Document'>\n**File Path:** /content/data/pdfcoffee.com-fpmkathy6th-edpdf.pdf\n**Content:**\n\n5.\n",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "### Full Context Being Sent to LLM: \nYou are a personal digital assistant named Samantha. I am Alexandr.                               \n Your task is to answer this prompt: [hello]                                \n You may use the following context only if it is necessary:         \n [CONVERSATION HISTORY: ]                                \n [USER FILES FETCHED: ]",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "## **Hello Alexandr! How can I assist you today?**",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Documents Information Being Sent to LLM: \n"
          ]
        },
        {
          "data": {
            "text/markdown": "### Document 1:\n**Type of doc:** <class 'langchain.schema.document.Document'>\n**File Path:** /content/data/NOTES Survey of Motion Pictures.pdf\n**Content:**\n\nA. First example of parallel action: multiple stories happening at the same time.\n\nB. Used black glass to not expose part of ﬁlm, rewind, expose that section to a diﬀerent\n\npiece of video.\n\nC. Example of camera motion:\n\nD. Hired 12 year old to paint ﬁlm cells.\n\nE. 1st closeup: shooting robber\n\nVII. Edison makes Motion pictures Patents Company:\n\nA. Edison wants to stop the competition\n\nB. Black markets and maﬁa spring up—> crime themes start up in movies\n\nVIII.Norman studios: Jacksonville Florida; only place making black cast ﬁlms\n\nIX. Miami:\n\nA. Tarzan made in Biltmore hotel\n\nX. New Jersey—>Florida, jacksonville—> cuba—> California\n\nXI. LA: still had rural, Indians, wild west themes\n\nA. Hollywood became exotic real estate because of the movie industry that sprung up\n\nthere\n\nXII. Broadway starts playing Long “Epic” foreign ﬁlms\n\nA. 1st: the Squaw Man\n\n1. 1 1/2 hours long, on 6 reels\n\n2. Takes from a Broadway place\n\n3. Generated ﬁrst Hollywood gossip about the aﬀair of main actress\n",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "### Document 2:\n**Type of doc:** <class 'langchain.schema.document.Document'>\n**File Path:** /content/data/NOTES Survey of Motion Pictures.pdf\n**Content:**\n\n(4) Discover truth, makes life worthwhile. What has the hero learned?\n\n(5) “Falling action” or the “Denouement”: you have to bring the audience down\n\n(6) We now know feeling: “the great ah”\n\nd) US vs Europe\n\n(1) Europe focuses on character\n\nPage\n\n5\n\nof\n\n12\n\n(2) US focuses on action\n\n(a) Likely successful because you don’t have to understand the language, as\n\nthe US was historically colonized and settled as a melting pot\n\nII. George Melies:\n\nA. Good at making sets and special eﬀects using models and costumes shown in HUGO\n\nB. Made ﬁlms like theater\n\nC. Prop usage\n\nD. Static camera\n\n1. Pushed props to ward the camera—like theater—camera stars across orchestra pit\n\nE. “A trip to the Moon”\n\n1. First ﬁlm about going to the moon\n\nIII. Edison steals a copy of “A trip to the moon”\n\nA. Discovers how to make copies of original movies\n\nB. Never pays Melies\n\nIV. Nickelodeons/ ﬂickers in US\n\nV. Bicycle prints: only one copy get shuttled around theaters on bicycles.\n",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "### Full Context Being Sent to LLM: \nYou are a personal digital assistant named Samantha. I am Alexandr.                               \n Your task is to answer this prompt: [Tell me about how miami was founded]                                \n You may use the following context only if it is necessary:         \n [CONVERSATION HISTORY:  Alexandr's input: 'hello' Your response: 'Hello Alexandr! How can I assist you today?']                                \n [USER FILES FETCHED: Document Number: [1] File Path: [/content/data/NOTES Survey of Motion Pictures.pdf] Content: [A. First example of parallel action: multiple stories happening at the same time.\n\nB. Used black glass to not expose part of ﬁlm, rewind, expose that section to a diﬀerent\n\npiece of video.\n\nC. Example of camera motion:\n\nD. Hired 12 year old to paint ﬁlm cells.\n\nE. 1st closeup: shooting robber\n\nVII. Edison makes Motion pictures Patents Company:\n\nA. Edison wants to stop the competition\n\nB. Black markets and maﬁa spring up—> crime themes start up in movies\n\nVIII.Norman studios: Jacksonville Florida; only place making black cast ﬁlms\n\nIX. Miami:\n\nA. Tarzan made in Biltmore hotel\n\nX. New Jersey—>Florida, jacksonville—> cuba—> California\n\nXI. LA: still had rural, Indians, wild west themes\n\nA. Hollywood became exotic real estate because of the movie industry that sprung up\n\nthere\n\nXII. Broadway starts playing Long “Epic” foreign ﬁlms\n\nA. 1st: the Squaw Man\n\n1. 1 1/2 hours long, on 6 reels\n\n2. Takes from a Broadway place\n\n3. Generated ﬁrst Hollywood gossip about the aﬀair of main actress]\nDocument Number: [2] File Path: [/content/data/NOTES Survey of Motion Pictures.pdf] Content: [(4) Discover truth, makes life worthwhile. What has the hero learned?\n\n(5) “Falling action” or the “Denouement”: you have to bring the audience down\n\n(6) We now know feeling: “the great ah”\n\nd) US vs Europe\n\n(1) Europe focuses on character\n\nPage\n\n5\n\nof\n\n12\n\n(2) US focuses on action\n\n(a) Likely successful because you don’t have to understand the language, as\n\nthe US was historically colonized and settled as a melting pot\n\nII. George Melies:\n\nA. Good at making sets and special eﬀects using models and costumes shown in HUGO\n\nB. Made ﬁlms like theater\n\nC. Prop usage\n\nD. Static camera\n\n1. Pushed props to ward the camera—like theater—camera stars across orchestra pit\n\nE. “A trip to the Moon”\n\n1. First ﬁlm about going to the moon\n\nIII. Edison steals a copy of “A trip to the moon”\n\nA. Discovers how to make copies of original movies\n\nB. Never pays Melies\n\nIV. Nickelodeons/ ﬂickers in US\n\nV. Bicycle prints: only one copy get shuttled around theaters on bicycles.]\n]",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "## **Based on the provided context, there is no information related to the founding of Miami. Therefore, I don't have the necessary information to answer the question.**",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Documents Information Being Sent to LLM: \n"
          ]
        },
        {
          "data": {
            "text/markdown": "### Document 1:\n**Type of doc:** <class 'langchain.schema.document.Document'>\n**File Path:** /Users/alexkim/Desktop/Research/Personal LLM Project/personal data for LLM/pdfcoffee.com-fpmkathy6th-edpdf copy.pdf\n**Content:**\n\nexample,\tthe\tfounder\tof\tSkype\twanted\tto\tmake\tthe\tworld\ta\tbetter\tplace, and\tSteve\tJobs\twanted\tto\tput\ta\tding\tin\tthe\tuniverse.\n",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "### Document 2:\n**Type of doc:** <class 'langchain.schema.document.Document'>\n**File Path:** /Users/alexkim/Desktop/Research/Personal LLM Project/personal data for LLM/pdfcoffee.com-fpmkathy6th-edpdf copy.pdf\n**Content:**\n\nexample,\tthe\tfounder\tof\tSkype\twanted\tto\tmake\tthe\tworld\ta\tbetter\tplace, and\tSteve\tJobs\twanted\tto\tput\ta\tding\tin\tthe\tuniverse.\n",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "### Full Context Being Sent to LLM: \nYou are a personal digital assistant named Samantha. I am Alexandr.                               \n Your task is to answer this prompt: [ignoring context, tell me about how miami was founded]                                \n You may use the following context only if it is necessary:         \n [CONVERSATION HISTORY:  Alexandr's input: 'hello' Your response: 'Hello Alexandr! How can I assist you today?' Alexandr's input: 'Tell me about how miami was founded' Your response: 'Based on the provided context, there is no information related to the founding of Miami. Therefore, I don't have the necessary information to answer the question.']                                \n [USER FILES FETCHED: ]",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "## **Based on the provided context, there is no information related to the founding of Miami. Therefore, I don't have the necessary information to answer the question.**",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-fb26133bc42d>\u001b[0m in \u001b[0;36m<cell line: 63>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Ask: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0msimilar_docs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtechnical_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_similar_docs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_answer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconversation_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtechnical_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "#1.2.1 USER INTERACTION LOGIC THAT ONLY RETURNS RELEVANT USER FILE CONTEXT:\n",
        "from IPython.display import Markdown, display\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "import re\n",
        "\n",
        "# Standard, immutable information\n",
        "name_of_user = \"Alexandr\"\n",
        "standard_info = f\"You are a personal digital assistant named Samantha. I am {name_of_user}\" + \".                               \"+ '\\n'\n",
        "\n",
        "def is_relevant(content, query):\n",
        "    content_tokens = set(re.findall(r'\\b\\w+\\b', content.lower()))\n",
        "    query_tokens = set(re.findall(r'\\b\\w+\\b', query.lower()))\n",
        "    return bool(content_tokens & query_tokens)\n",
        "\n",
        "def get_similar_docs(query, k=2, score=False):\n",
        "    if score:\n",
        "        similar_docs = index.similarity_search_with_score(query, k=k)\n",
        "    else:\n",
        "        similar_docs = index.similarity_search(query, k=k)\n",
        "\n",
        "    # Initialize technical context\n",
        "    technical_context = \"\"\n",
        "\n",
        "    print(\"Documents Information Being Sent to LLM: \")\n",
        "    for i, doc in enumerate(similar_docs, start=1):\n",
        "        doc_type = f\"**Type of doc:** {type(doc)}\"\n",
        "        content = getattr(doc, 'page_content', 'Content not available')\n",
        "        file_path = doc.metadata.get('source', 'File path not available') if hasattr(doc, 'metadata') else 'Metadata not available'\n",
        "\n",
        "        formatted_content = f\"**Content:**\\n\\n{content}\\n\"\n",
        "        formatted_file_path = f\"**File Path:** {file_path}\\n\"\n",
        "\n",
        "        # Append document number, file path, and content to technical context with clear labels and separators\n",
        "        technical_context += f\"Document Number: [{i}] File Path: [{file_path}] Content: [{content}]\\n\"\n",
        "\n",
        "        display(Markdown(f\"### Document {i}:\\n{doc_type}\\n{formatted_file_path}{formatted_content}\"))\n",
        "\n",
        "    return similar_docs, technical_context\n",
        "\n",
        "def get_answer(chain, query, conversation_history, technical_context):\n",
        "    formatted_query = f\"Your task is to answer this prompt: [{query}]                                \\n You may use the following context only if it is necessary:         \" + '\\n'\n",
        "\n",
        "    # Check the relevance of technical context and conversation history\n",
        "    relevant_technical_context = technical_context if is_relevant(technical_context, query) else \"\"\n",
        "    relevant_conversation_history = conversation_history if is_relevant(conversation_history, query) else \"\"\n",
        "\n",
        "    # Format the different parts of the context with labels and enclose them in square brackets\n",
        "    formatted_conversation_history = f\"[CONVERSATION HISTORY: {relevant_conversation_history}]                                \\n\"\n",
        "    formatted_technical_context = f\"[USER FILES FETCHED: {relevant_technical_context}]\"\n",
        "\n",
        "    full_context = f\"{standard_info} {formatted_query} {formatted_conversation_history} {formatted_technical_context}\"\n",
        "\n",
        "    display(Markdown(f\"### Full Context Being Sent to LLM: \\n{full_context}\"))\n",
        "\n",
        "    answer = chain.run(input_documents=similar_docs, question=full_context)\n",
        "    return answer\n",
        "\n",
        "# Load the language model and the question-answering chain\n",
        "chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
        "\n",
        "conversation_history = \"\"\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"Ask: \")\n",
        "    similar_docs, technical_context = get_similar_docs(user_input)\n",
        "    answer = get_answer(chain, user_input, conversation_history, technical_context)\n",
        "    display(Markdown(f\"## **{answer}**\"))\n",
        "    conversation_history += f\" {name_of_user}'s input: '{user_input}' Your response: '{answer}'\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "bNY58Co7sWzI",
        "outputId": "7c86d4da-be67-4822-ab88-531a5e63da91"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-f29df42898b9>\u001b[0m in \u001b[0;36m<cell line: 54>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Ask: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m# Get similar docs and extract their content, document number, and file path as technical context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "#1.2 USER INTERACTION LOGIC WITH REFINED SESSION CONTEXT:\n",
        "from IPython.display import Markdown, display\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "\n",
        "# Standard, immutable information\n",
        "name_of_user = \"Alexandr\"\n",
        "standard_info = \"You are a personal digital assistant named Samantha. I am \" +name_of_user+ \".                                \"+ '\\n'\n",
        "\n",
        "def get_similar_docs(query, k=2, score=False):\n",
        "    if score:\n",
        "        similar_docs = index.similarity_search_with_score(query, k=k)\n",
        "    else:\n",
        "        similar_docs = index.similarity_search(query, k=k)\n",
        "\n",
        "    # Initialize technical context\n",
        "    technical_context = \"\"\n",
        "\n",
        "    print(\"Documents Information Being Sent to LLM: \")\n",
        "    for i, doc in enumerate(similar_docs, start=1):\n",
        "        doc_type = f\"**Type of doc:** {type(doc)}\"\n",
        "        content = getattr(doc, 'page_content', 'Content not available')\n",
        "        file_path = doc.metadata.get('source', 'File path not available') if hasattr(doc, 'metadata') else 'Metadata not available'\n",
        "\n",
        "        formatted_content = f\"**Content:**\\n\\n{content}\\n\"\n",
        "        formatted_file_path = f\"**File Path:** {file_path}\\n\"\n",
        "\n",
        "        # Append document number, file path, and content to technical context with clear labels and separators\n",
        "        technical_context += f\"Document Number: [{i}] File Path: [{file_path}] Content: [{content}]\\n\"\n",
        "\n",
        "        display(Markdown(f\"### Document {i}:\\n{doc_type}\\n{formatted_file_path}{formatted_content}\"))\n",
        "\n",
        "    return similar_docs, technical_context\n",
        "\n",
        "def get_answer(chain, query, conversation_history, technical_context):\n",
        "    # Format the different parts of the context with labels and enclose them in square brackets\n",
        "    formatted_query = f\"Your task is to answer this prompt: [{query}]                                \\n You may use the following information only if it is necessary:         \" + '\\n'\n",
        "    formatted_conversation_history = f\"[CONVERSATION HISTORY: {conversation_history}]                                \\n\"\n",
        "    formatted_technical_context = f\"[USER FILES FETCHED: {technical_context}]\"\n",
        "\n",
        "    # Combine standard info, formatted query, formatted conversation history, and formatted technical context\n",
        "    full_context = f\"{standard_info} {formatted_query} {formatted_conversation_history} {formatted_technical_context}\"\n",
        "\n",
        "    # Print and display the full context being sent to the LLM\n",
        "    display(Markdown(f\"### Full Context Being Sent to LLM: \\n{full_context}\"))\n",
        "\n",
        "    answer = chain.run(input_documents=similar_docs, question=full_context)\n",
        "    return answer\n",
        "\n",
        "# Load the language model and the question-answering chain\n",
        "chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
        "\n",
        "conversation_history = \"\"\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"Ask: \")\n",
        "\n",
        "    # Get similar docs and extract their content, document number, and file path as technical context\n",
        "    similar_docs, technical_context = get_similar_docs(user_input)\n",
        "\n",
        "    answer = get_answer(chain, user_input, conversation_history, technical_context)\n",
        "    print('\\n')\n",
        "    display(Markdown(f\"## **{answer}**\"))\n",
        "\n",
        "    conversation_history += f\" {name_of_user}'s Input: \\'{user_input}\\'     \\n Samantha Response: \\'{answer}\\'     \\n\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "XWQ_r8iMR-zL",
        "outputId": "3ae77991-e74c-48b7-b808-1db112e61162"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-6d7370b80015>\u001b[0m in \u001b[0;36m<cell line: 41>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mconversation_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Ask: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0muser_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"You are an academic LLM assistant named Samantha. I am Alexandr. Answer the following prompt: {conversation_history} {user_input}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_answer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "#1.1 USER INTERACTION LOGIC WITH SESSION CONTEXT AND APPEALING DOCUMENT CONTEXT PRINT STATEMENT:\n",
        "from IPython.display import Markdown, display\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "\n",
        "def get_similar_docs(query, k=3, score=False):\n",
        "    if score:\n",
        "        similar_docs = index.similarity_search_with_score(query, k=k)\n",
        "    else:\n",
        "        similar_docs = index.similarity_search(query, k=k)\n",
        "\n",
        "    # Print out the documents' information being sent to the LLM\n",
        "    print(\"Documents Information Being Sent to LLM: \")\n",
        "    for i, doc in enumerate(similar_docs, start=1):\n",
        "        # Accessing the type, content, and file path of the document\n",
        "        doc_type = f\"**Type of doc:** {type(doc)}\"\n",
        "        content = getattr(doc, 'page_content', 'Content not available')\n",
        "        file_path = doc.metadata.get('source', 'File path not available') if hasattr(doc, 'metadata') else 'Metadata not available'\n",
        "\n",
        "        formatted_content = f\"**Content:**\\n\\n{content}\\n\"\n",
        "        formatted_file_path = f\"**File Path:** {file_path}\\n\"\n",
        "\n",
        "        # Display the formatted document information\n",
        "        display(Markdown(f\"### Document {i}:\\n{doc_type}\\n{formatted_file_path}{formatted_content}\"))\n",
        "\n",
        "    return similar_docs\n",
        "\n",
        "def get_answer(chain, query):\n",
        "    similar_docs = get_similar_docs(query)\n",
        "\n",
        "    # Print out the question being sent to the LLM\n",
        "    print(\"Question Being Sent to LLM: \")\n",
        "    display(Markdown(f\"### Question Being Sent to LLM \\n{query}\"))\n",
        "\n",
        "    answer = chain.run(input_documents=similar_docs, question=query)\n",
        "    return answer\n",
        "\n",
        "# Load the language model and the question-answering chain\n",
        "chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
        "\n",
        "conversation_history = \"\"\n",
        "while True:\n",
        "    user_input = input(\"Ask: \")\n",
        "    user_query = f\"You are an academic LLM assistant named Samantha. I am Alexandr. Answer the following prompt: {conversation_history} {user_input}\"\n",
        "    answer = get_answer(chain, user_query)\n",
        "    display(Markdown(answer))\n",
        "    # Append the user input and model response to the conversation history.\n",
        "    conversation_history += f\"User: {user_input}\\nSamantha: {answer}\\n\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "RqCE-C3Ubty0",
        "outputId": "949886db-e2d1-49be-e1d0-8d61b465fc08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ask: Hello\n",
            "[Document(page_content='Discussion\\tQuestions Exercises End\\tNotes', metadata={'source': '/Users/alexkim/Desktop/Research/Personal LLM Project/personal data for LLM/pdfcoffee.com-fpmkathy6th-edpdf copy.pdf'}), Document(page_content='Discussion\\tQuestions Exercises End\\tNotes', metadata={'source': '/Users/alexkim/Desktop/Research/Personal LLM Project/personal data for LLM/pdfcoffee.com-fpmkathy6th-edpdf copy.pdf'}), Document(page_content='Discussion\\tQuestions Exercises End\\tNotes', metadata={'source': '/Users/alexkim/Desktop/Research/Personal LLM Project/personal data for LLM/pdfcoffee.com-fpmkathy6th-edpdf copy.pdf'}), Document(page_content='Discussion\\tQuestions Exercises End\\tNotes', metadata={'source': '/content/data/pdfcoffee.com-fpmkathy6th-edpdf.pdf'})]\n"
          ]
        },
        {
          "data": {
            "text/markdown": "Hello Alexandr Kim! How can I assist you today?",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ask: Write a haiku about PM\n",
            "[Document(page_content='those that focus on benefits realization. Write a short paper or prepare a presentation summarizing your findings. Write a short paper or prepare a presentation summarizing your views of your personal leadership style and experience. How do you think people can improve their leadership skills? Research information about earning and maintaining PMP® and CAPM® certifications. See the Links section of www.intropm.com for some references. Summarize your findings in a short paper or presentation.', metadata={'source': '/content/data/Textbook Fundamentals of Tech Project Management.pdf'}), Document(page_content='those that focus on benefits realization. Write a short paper or prepare a presentation summarizing your findings. Write a short paper or prepare a presentation summarizing your views of your personal leadership style and experience. How do you think people can improve their leadership skills? Research information about earning and maintaining PMP® and CAPM® certifications. See the Links section of www.intropm.com for some references. Summarize your findings in a short paper or presentation.', metadata={'source': '/content/data/Textbook Fundamentals of Tech Project Management.pdf'}), Document(page_content='34\\n\\nChapter 1 – Introduction (Copyright 2017 Schwalbe Publishing)\\n\\n2.\\n\\n3.\\n\\n4.\\n\\nGo to www.indeed.com or another job search site and search for jobs as a \"project manager\" or “program manager” in three geographic regions of your choice. Write a one- to two-page paper or prepare a short presentation summarizing what you found. As a team, discuss projects that you are currently working on or would like to work on to benefit yourself, your employers, your family, or the broader community. Come up with at least ten projects, and then determine if they could be grouped into programs. Write a one- to two-page paper or prepare a short presentation summarizing your results. Review information about the exams required for earning PMP® and CAPM® certification. Find and take several sample tests. Document your findings in a one- to two-page paper or short presentation, citing your references.', metadata={'source': '/content/data/Textbook Fundamentals of Tech Project Management.pdf'}), Document(page_content='34\\n\\nChapter 1 – Introduction (Copyright 2017 Schwalbe Publishing)\\n\\n2.\\n\\n3.\\n\\n4.\\n\\nGo to www.indeed.com or another job search site and search for jobs as a \"project manager\" or “program manager” in three geographic regions of your choice. Write a one- to two-page paper or prepare a short presentation summarizing what you found. As a team, discuss projects that you are currently working on or would like to work on to benefit yourself, your employers, your family, or the broader community. Come up with at least ten projects, and then determine if they could be grouped into programs. Write a one- to two-page paper or prepare a short presentation summarizing your results. Review information about the exams required for earning PMP® and CAPM® certification. Find and take several sample tests. Document your findings in a one- to two-page paper or short presentation, citing your references.', metadata={'source': '/content/data/Textbook Fundamentals of Tech Project Management.pdf'})]\n"
          ]
        },
        {
          "data": {
            "text/markdown": "I don't know.",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ask: Can you understand context?\n",
            "[Document(page_content='For\\texample,\\tif\\tyou\\tare\\treading\\tthis\\ttext\\tas\\tpart\\tof\\ta\\tcourse,\\tyou', metadata={'source': '/Users/alexkim/Desktop/Research/Personal LLM Project/personal data for LLM/pdfcoffee.com-fpmkathy6th-edpdf copy.pdf'}), Document(page_content='For\\texample,\\tif\\tyou\\tare\\treading\\tthis\\ttext\\tas\\tpart\\tof\\ta\\tcourse,\\tyou', metadata={'source': '/Users/alexkim/Desktop/Research/Personal LLM Project/personal data for LLM/pdfcoffee.com-fpmkathy6th-edpdf copy.pdf'}), Document(page_content='For\\texample,\\tif\\tyou\\tare\\treading\\tthis\\ttext\\tas\\tpart\\tof\\ta\\tcourse,\\tyou', metadata={'source': '/Users/alexkim/Desktop/Research/Personal LLM Project/personal data for LLM/pdfcoffee.com-fpmkathy6th-edpdf copy.pdf'}), Document(page_content='OPENING\\tCASE Marie\\tScott,\\tthe\\tdirector\\tof\\tthe\\tProject\\tManagement\\tOffice\\tfor\\tGlobal Construction,\\tInc.,\\twas\\tfacilitating\\ta\\tmeeting\\twith\\tseveral\\tsenior\\tmanagers throughout\\tthe\\tcompany.\\tThe\\tpurpose\\tof\\tthe\\tmeeting\\twas\\tto\\tdiscuss\\ta process\\tfor\\tselecting\\tprojects,\\tgrouping\\tthem\\tinto\\tprograms,\\tand\\tdetermining how\\tthey\\tfit\\tinto\\tthe\\torganization’s\\tportfolio\\tof\\tprojects.\\tShe\\thad\\tinvited\\tan outside\\tconsultant\\tto\\tthe\\tmeeting\\tto\\tprovide\\tan\\tobjective\\tview\\tof\\tthe\\tprocess.\\n\\nShe\\tcould\\tsee\\tthat\\tseveral\\tmanagers\\twere\\tgetting\\tbored\\twith\\tthe\\n\\npresentation,\\twhile\\tothers\\tlooked\\tconcerned\\tthat\\ttheir\\tprojects\\tmight\\tbe cancelled\\tif\\tthe\\tcompany\\timplemented\\ta\\tnew\\tapproach\\tfor\\tproject\\tselection. After\\tthe\\tconsultant’s\\tpresentation,\\tMarie\\thad\\teach\\tparticipant\\twrite\\tdown his\\tor\\ther\\tquestions\\tand\\tconcerns\\tand\\thand\\tthem\\tin\\tanonymously\\tfor\\ther group\\tto\\treview.\\tShe\\twas\\tamazed\\tat\\tthe\\tobvious\\tlack\\tof\\tunderstanding\\tof\\tthe need\\tfor\\tprojects\\tto\\talign\\twith\\tbusiness\\tstrategy.\\tHow\\tshould\\tMarie\\trespond?', metadata={'source': '/content/data/pdfcoffee.com-fpmkathy6th-edpdf.pdf'})]\n"
          ]
        },
        {
          "data": {
            "text/markdown": "As an AI, I can understand and provide information based on the context given in the text. However, I do not have personal experiences or knowledge of specific individuals, such as an academic LLM assistant named Samantha or an individual named Alexandr Kim.",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-13ee71f89e4a>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0muser_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"You are an academic LLM assistant named Samantha. I am Alexandr Kim. Answer the following prompt: \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Ask: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_answer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "#1.0 ORIGINAL USER INTERACTION LOGIC:\n",
        "from IPython.display import Markdown, display\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "\n",
        "def get_similiar_docs(query,k=4,score=False):\n",
        "  if score:\n",
        "    similar_docs = index.similarity_search_with_score(query,k=k)\n",
        "  else:\n",
        "    similar_docs = index.similarity_search(query,k=k)\n",
        "  print(similar_docs)\n",
        "  return similar_docs\n",
        "\n",
        "\n",
        "def get_answer(chain, query):\n",
        "    similar_docs = get_similiar_docs(query)\n",
        "    answer = chain.run(input_documents=similar_docs, question=query)\n",
        "    return answer\n",
        "\n",
        "# Load the language model and the question-answering chain\n",
        "# Make sure to replace 'llm' with the appropriate model or language model object.\n",
        "chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
        "\n",
        "while True:\n",
        "    user_query = \"You are an academic LLM assistant named Samantha. I am Alexandr Kim. Answer the following prompt: \"+input(\"Ask: \")\n",
        "\n",
        "    answer = get_answer(chain, user_query)\n",
        "    display(Markdown(answer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyX0gsP_IYsb"
      },
      "outputs": [],
      "source": [
        "#1.2.2 FAILED/ WIP__________________________USER INTERACTION LOGIC THAT RETURNS RELEVANT USER FILE CONTEXT MORE SOPHISTICATED IS_RELEVANT FUNCTION:\n",
        "!pip install nltk sentence-transformers scikit-learn\n",
        "\n",
        "\n",
        "from IPython.display import Markdown, display\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "import re\n",
        "import tensorflow_hub as hub\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Standard, immutable information\n",
        "name_of_user = \"Alexandr\"\n",
        "standard_info = f\"You are a personal digital assistant named Samantha. I am {name_of_user}\"+ \".                                \"+ '\\n'\n",
        "\n",
        "# Tokenization and Stopword removal\n",
        "def tokenize_and_remove_stopwords(text):\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
        "    return [word for word in tokens if word.lower() not in stopwords]\n",
        "\n",
        "# Keyword Matching\n",
        "def keyword_matching(query, document):\n",
        "    query_tokens = set(tokenize_and_remove_stopwords(query))\n",
        "    document_tokens = set(tokenize_and_remove_stopwords(document))\n",
        "    return bool(query_tokens & document_tokens)  # & operator performs intersection on sets\n",
        "\n",
        "# Load a pre-trained Sentence Transformer model\n",
        "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "\n",
        "# Compute Semantic Similarity\n",
        "def compute_semantic_similarity(query, document):\n",
        "    query_embedding = model.encode([query])\n",
        "    document_embedding = model.encode([document])\n",
        "    return cosine_similarity(query_embedding, document_embedding)[0][0]\n",
        "\n",
        "# Hybrid Relevance Check\n",
        "def is_relevant(query, document, threshold=0.7):\n",
        "    if not keyword_matching(query, document):\n",
        "        return False\n",
        "    similarity = compute_semantic_similarity(query, document)\n",
        "    return similarity >= threshold\n",
        "\n",
        "def get_similar_docs(query, k=2, score=False):\n",
        "    if score:\n",
        "        similar_docs = index.similarity_search_with_score(query, k=k)\n",
        "    else:\n",
        "        similar_docs = index.similarity_search(query, k=k)\n",
        "\n",
        "    # Initialize technical context\n",
        "    technical_context = \"\"\n",
        "\n",
        "    print(\"Documents Information Being Sent to LLM: \")\n",
        "    for i, doc in enumerate(similar_docs, start=1):\n",
        "        doc_type = f\"**Type of doc:** {type(doc)}\"\n",
        "        content = getattr(doc, 'page_content', 'Content not available')\n",
        "        file_path = doc.metadata.get('source', 'File path not available') if hasattr(doc, 'metadata') else 'Metadata not available'\n",
        "\n",
        "        formatted_content = f\"**Content:**\\n\\n{content}\\n\"\n",
        "        formatted_file_path = f\"**File Path:** {file_path}\\n\"\n",
        "\n",
        "        # Append document number, file path, and content to technical context with clear labels and separators\n",
        "        technical_context += f\"Document Number: [{i}] File Path: [{file_path}] Content: [{content}]\\n\"\n",
        "\n",
        "        display(Markdown(f\"### Document {i}:\\n{doc_type}\\n{formatted_file_path}{formatted_content}\"))\n",
        "\n",
        "    return similar_docs, technical_context\n",
        "\n",
        "def get_answer(chain, query, conversation_history, technical_context):\n",
        "    formatted_query = f\"Your task is to answer this prompt: [{query}]                                \\n You may use the following context only if it is necessary:         \" + '\\n'\n",
        "\n",
        "    # Check the relevance of technical context and conversation history\n",
        "    relevant_technical_context = technical_context if is_relevant(technical_context, query) else \"\"\n",
        "    relevant_conversation_history = conversation_history if is_relevant(conversation_history, query) else \"\"\n",
        "\n",
        "    # Format the different parts of the context with labels and enclose them in square brackets\n",
        "    formatted_conversation_history = f\"[CONVERSATION HISTORY: {relevant_conversation_history}]                                \\n\"\n",
        "    formatted_technical_context = f\"[USER FILES FETCHED: {relevant_technical_context}]\"\n",
        "\n",
        "    full_context = f\"{standard_info} {formatted_query} {formatted_conversation_history} {formatted_technical_context}\"\n",
        "\n",
        "    display(Markdown(f\"### Full Context Being Sent to LLM: \\n{full_context}\"))\n",
        "\n",
        "    answer = chain.run(input_documents=similar_docs, question=full_context)\n",
        "    return answer\n",
        "\n",
        "# Load the language model and the question-answering chain\n",
        "chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
        "\n",
        "conversation_history = \"\"\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"Ask: \")\n",
        "    similar_docs, technical_context = get_similar_docs(user_input)\n",
        "    answer = get_answer(chain, user_input, conversation_history, technical_context)\n",
        "    display(Markdown(f\"## **{answer}**\"))\n",
        "    conversation_history += f\" {name_of_user}'s Input: '{user_input}' Samantha Response: '{answer}'\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QTViqc06xBh4"
      },
      "outputs": [],
      "source": [
        "#1.3 USER INTERACTION LOGIC WITH DATE AND TIME + INTERNET ACCESS:\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}